# Model Architecture (TinyStories Configuration)
vocab_size: 10000          # As specified in instructions
context_length: 256        # As specified in instructions
d_model: 512              # As specified in instructions
num_layers: 4             # As specified in instructions
num_heads: 16             # As specified in instructions
d_ff: 1344               # As specified in instructions (8/3 * d_model)
rope_theta: 10000.0      # As specified in instructions

# Data Paths
train_data_path: "data/TinyStoriesV2-GPT4-train.txt"  # Your data path
val_data_path: "data/TinyStoriesV2-GPT4-valid.txt"       # No validation data specified

# device
device: "mps"

# Training Parameters (to be tuned)
batch_size: 32            # You'll need to tune this
learning_rate: 0.0001     # 1e-4 as float
betas: [0.9, 0.999]      # Typical AdamW values
eps: 0.00000001          # 1e-8 as float
weight_decay: 0.01       # You'll need to tune this
grad_clip_norm: 1.0      # You'll need to tune this

# Learning Rate Schedule (to be tuned)
use_lr_schedule: true
max_learning_rate: 0.001  # 1e-3 as float
min_learning_rate: 0.00001 # 1e-5 as float
warmup_iters: 1000        # You'll need to tune this
cosine_cycle_iters: 10000 # You'll need to tune this

# Total Tokens Target
# Your batch_size × total_steps × context_length should equal ~327,680,000
# With batch_size=32 and context_length=256, you need ~40,000 steps
num_epochs: 100           # Adjust based on your dataset size

# Logging Configuration
log_interval: 100         # Log every N steps
checkpoint_interval: 10   # Save checkpoint every N epochs
checkpoint_dir: "checkpoints"  # Directory to save checkpoints

# Logging Configuration for Wandb
logging:
  experiment_name: "transformer_lm_training"
  project_name: "cs336-basics"
  tags: ["transformer", "language-model", "training"]
  log_training_every: 100
  log_gradient_norms: true
  log_learning_rate: true
  log_memory_usage: true
  log_compute_time: true
